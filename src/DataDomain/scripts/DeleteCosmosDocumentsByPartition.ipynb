#!/usr/bin/env python
# coding: utf-8

# ## DeleteCosmosDocumentsByPartition
# 
# 
# 

# ## DeleteCosmosDocumentsByPartition
# 
# 
# 

# In[9]:


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._

case class TimeRange(startTime: Option[String], endTime: Option[String])

trait CosmosConfig {
  val cosmosEndpoint: String
  val cosmosDatabase: String 
  val cosmosContainer: String
  val partitionKey: String
  val timeColumn: Option[String]
  def getCosmosKey: String
}

case class CosmosDBConfig(
  endpoint: String,
  database: String,
  container: String,
  key: String,
  partitionKeyColumn: String,
  timeColumnName: Option[String] = None
) extends CosmosConfig {
  override val cosmosEndpoint = endpoint
  override val cosmosDatabase = database
  override val cosmosContainer = container
  override val partitionKey = partitionKeyColumn
  override val timeColumn = timeColumnName
  override def getCosmosKey = key
}

trait DataService {
  def retrieveRecordsByPartitionKey(partitionValue: String, timeRange: Option[TimeRange] = None): DataFrame
  def deleteRecords(records: DataFrame): Unit
}

class CosmosDBService(spark: SparkSession, config: CosmosConfig) extends DataService {
  private def getCosmosReader = {
    spark.read.format("cosmos.olap")
      .option("spark.cosmos.accountEndpoint", config.cosmosEndpoint)
      .option("spark.cosmos.database", config.cosmosDatabase)
      .option("spark.cosmos.container", config.cosmosContainer)
      .option("spark.cosmos.accountKey", config.getCosmosKey)
  }
  
  private def getCosmosWriter(df: DataFrame) = {
    df.write
      .format("cosmos.oltp")
      .option("spark.cosmos.accountEndpoint", config.cosmosEndpoint)
      .option("spark.cosmos.accountKey", config.getCosmosKey)
      .option("spark.cosmos.database", config.cosmosDatabase)
      .option("spark.cosmos.container", config.cosmosContainer)
      .option("spark.cosmos.write.strategy", "ItemDelete")
      .mode("Append")
  }

  override def retrieveRecordsByPartitionKey(partitionValue: String, timeRange: Option[TimeRange] = None): DataFrame = {
    var query = getCosmosReader.load().filter(col(config.partitionKey) === partitionValue)
    
    (config.timeColumn, timeRange) match {
      case (Some(timeCol), Some(TimeRange(Some(start), Some(end)))) =>
        query = query.filter(col(timeCol).between(start, end))
      case (Some(timeCol), Some(TimeRange(Some(start), None))) =>
        query = query.filter(col(timeCol) >= start)
      case (Some(timeCol), Some(TimeRange(None, Some(end)))) =>
        query = query.filter(col(timeCol) <= end)
      case _ => query
    }
    query
  }

  override def deleteRecords(records: DataFrame): Unit = {
    try {
      val count = records.count()
      println(s"Total records to be deleted: $count")
      getCosmosWriter(records).save()
      println(s"$count records deleted successfully")
    } catch {
      case e: Exception =>
        println(s"Error deleting records: ${e.getMessage}")
        throw e
    }
  }
}

class DeletionExecutor(service: DataService, config: CosmosConfig) {
  
  def getRecordsToDelete(partitionValue: String, timeRange: Option[TimeRange] = None): DataFrame = {
    println(s"1. Retrieving records for partition value: $partitionValue")
    val recordsToDelete = service.retrieveRecordsByPartitionKey(partitionValue, timeRange)
    println("\n2. Records to be deleted:")
    val displayColumns = Seq("id", config.partitionKey) ++ config.timeColumn.toSeq
    recordsToDelete.select(displayColumns.map(col): _*).show(false)
    recordsToDelete
  }

  def executePartitionDeletion(recordsToDelete:DataFrame): Unit = {
    println("\n3. Executing deletion...")
    service.deleteRecords(recordsToDelete)
  }
}


# In[10]:


val config = CosmosDBConfig(
    endpoint = "https://pdgprodcosmosdocdxb.documents.azure.com:443/",
    database = "dgh-Control",
    container = "DHScore",
    key = mssparkutils.credentials.getSecret("dgh-prod-dxb-kv","cosmosDBWritekey"),
    partitionKeyColumn = "TenantId",
    timeColumnName = Some("Time")
)
//val timeRange = null
// // With time range
val timeRange = Some(TimeRange(
     startTime = Some("2024-08-27"),
     endTime = Some("2024-11-30")
 ))

val service = new CosmosDBService(spark, config)
val executor = new DeletionExecutor(service, config)
val recordsToDelete = executor.getRecordsToDelete("0ee21e60-cbf6-4137-af1e-6e2d8b81c72f", timeRange)
display(recordsToDelete)


# In[11]:


recordsToDelete.count()


# In[12]:


executor.executePartitionDeletion(recordsToDelete)


# In[6]:


val recordsToDelete = executor.getRecordsToDelete("haris-test-tenant", timeRange)
display(recordsToDelete)


# In[ ]:





