{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "name": "scala"
    },
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "testCatalog",
              "statement_id": 5,
              "statement_ids": [
                5
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "7",
              "normalized_state": "finished",
              "queued_time": "2025-03-24T18:08:21.9426487Z",
              "session_start_time": null,
              "execution_start_time": "2025-03-24T18:08:21.9453734Z",
              "execution_finish_time": "2025-03-24T18:08:22.6729639Z",
              "parent_msg_id": "c061a874-6f66-40a5-94e3-54b8bebb2a4b"
            },
            "text/plain": "StatementMeta(testCatalog, 7, 5, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import org.apache.spark.sql.{SparkSession, DataFrame}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.types._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "testCatalog",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "7",
              "normalized_state": "finished",
              "queued_time": "2025-03-24T18:08:21.9976803Z",
              "session_start_time": null,
              "execution_start_time": "2025-03-24T18:08:22.6778769Z",
              "execution_finish_time": "2025-03-24T18:08:23.9787601Z",
              "parent_msg_id": "d1d46c18-03bd-41ef-a5cf-0a979fde0260"
            },
            "text/plain": "StatementMeta(testCatalog, 7, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cdeContractSchema: org.apache.spark.sql.types.StructType = StructType(StructField(payload,StructType(StructField(before,StructType(StructField(name,StringType,true),StructField(dataType,StringType,true),StructField(status,StringType,true),StructField(contacts,StructType(StructField(owner,ArrayType(StructType(StructField(id,StringType,true)),true),true)),true),StructField(description,StringType,true),StructField(domain,StringType,true),StructField(id,StringType,true),StructField(systemData,StructType(StructField(lastModifiedAt,StringType,true),StructField(lastModifiedBy,StringType,true),StructField(createdAt,StringType,true),StructField(createdBy,StringType,true)),true)),true),StructField(after,StructType(StructField(name,StringType,true),StructField(dataType,StringType,true),StructField...\nobjectiveContractSchema: org.apache.spark.sql.types.StructType = StructType(StructField(payload,StructType(StructField(before,StructType(StructField(id,StringType,true),StructField(definition,StringType,true),StructField(domain,StringType,true),StructField(targetDate,StringType,true),StructField(contacts,StructType(StructField(owner,ArrayType(StructType(StructField(id,StringType,true)),true),true)),true),StructField(status,StringType,true),StructField(systemData,StructType(StructField(lastModifiedAt,StringType,true),StructField(lastModifiedBy,StringType,true),StructField(createdAt,StringType,true),StructField(createdBy,StringType,true)),true)),true),StructField(after,StructType(StructField(id,StringType,true),StructField(definition,StringType,true),StructField(domain,StringType,true),St...\nkeyResultContractSchema: org.apache.spark.sql.types.StructType = StructType(StructField(payload,StructType(StructField(before,StructType(StructField(id,StringType,true),StructField(definition,StringType,true),StructField(domainId,StringType,true),StructField(progress,IntegerType,true),StructField(goal,IntegerType,true),StructField(max,IntegerType,true),StructField(status,StringType,true),StructField(systemData,StructType(StructField(lastModifiedAt,StringType,true),StructField(lastModifiedBy,StringType,true),StructField(createdAt,StringType,true),StructField(createdBy,StringType,true)),true)),true),StructField(after,StructType(StructField(id,StringType,true),StructField(definition,StringType,true),StructField(domainId,StringType,true),StructField(progress,IntegerType,true),StructField(go...\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "val cdeContractSchema: StructType = StructType(Seq(\n",
        "  StructField(\"payload\", StructType(Seq(\n",
        "    StructField(\"before\", StructType(Seq(\n",
        "      StructField(\"name\", StringType),\n",
        "      StructField(\"dataType\", StringType),\n",
        "      StructField(\"status\", StringType),\n",
        "      StructField(\"contacts\", StructType(Seq(\n",
        "        StructField(\"owner\", ArrayType(StructType(Seq(\n",
        "          StructField(\"id\", StringType)\n",
        "        ))))\n",
        "      ))),\n",
        "      StructField(\"description\", StringType),\n",
        "      StructField(\"domain\", StringType),\n",
        "      StructField(\"id\", StringType),\n",
        "      StructField(\"systemData\", StructType(Seq(\n",
        "        StructField(\"lastModifiedAt\", StringType),\n",
        "        StructField(\"lastModifiedBy\", StringType),\n",
        "        StructField(\"createdAt\", StringType),\n",
        "        StructField(\"createdBy\", StringType)\n",
        "      )))\n",
        "    )), nullable = true),  // Set before field as nullable\n",
        "    StructField(\"after\", StructType(Seq(\n",
        "      StructField(\"name\", StringType),\n",
        "      StructField(\"dataType\", StringType),\n",
        "      StructField(\"status\", StringType),\n",
        "      StructField(\"contacts\", StructType(Seq(\n",
        "        StructField(\"owner\", ArrayType(StructType(Seq(\n",
        "          StructField(\"id\", StringType)\n",
        "        ))))\n",
        "      ))),\n",
        "      StructField(\"description\", StringType),\n",
        "      StructField(\"domain\", StringType),\n",
        "      StructField(\"id\", StringType),\n",
        "      StructField(\"systemData\", StructType(Seq(\n",
        "        StructField(\"lastModifiedAt\", StringType),\n",
        "        StructField(\"lastModifiedBy\", StringType),\n",
        "        StructField(\"createdAt\", StringType),\n",
        "        StructField(\"createdBy\", StringType)\n",
        "      )))\n",
        "    )), nullable = true),  // Set after field as nullable\n",
        "    StructField(\"related\", StructType(Seq(\n",
        "      StructField(\"dataAssetId\", ArrayType(StringType))\n",
        "    )), nullable = true)  // Also making related nullable for consistency\n",
        "  ))),\n",
        "  StructField(\"eventSource\", StringType),\n",
        "  StructField(\"payloadKind\", StringType),\n",
        "  StructField(\"operationType\", StringType),\n",
        "  StructField(\"preciseTimestamp\", StringType),\n",
        "  StructField(\"tenantId\", StringType),\n",
        "  StructField(\"accountId\", StringType),\n",
        "  StructField(\"changedBy\", StringType),\n",
        "  StructField(\"eventId\", StringType),\n",
        "  StructField(\"correlationId\", StringType),\n",
        "  StructField(\"EventProcessedUtcTime\", StringType),\n",
        "  StructField(\"PartitionId\", IntegerType),\n",
        "  StructField(\"EventEnqueuedUtcTime\", StringType),\n",
        "  StructField(\"id\", StringType),\n",
        "  StructField(\"_rid\", StringType),\n",
        "  StructField(\"_etag\", StringType),\n",
        "  StructField(\"_ts\", LongType)\n",
        "))\n",
        "\n",
        "\n",
        "val objectiveContractSchema : StructType = StructType(Seq(\n",
        "    StructField(\"payload\", StructType(Seq(\n",
        "      StructField(\"before\", StructType(Seq(\n",
        "        StructField(\"id\", StringType),\n",
        "        StructField(\"definition\", StringType),\n",
        "        StructField(\"domain\", StringType),\n",
        "        StructField(\"targetDate\", StringType),\n",
        "        StructField(\"contacts\", StructType(Seq(\n",
        "          StructField(\"owner\", ArrayType(StructType(Seq(\n",
        "            StructField(\"id\", StringType)\n",
        "          )))),\n",
        "        ))),\n",
        "        StructField(\"status\", StringType),\n",
        "        StructField(\"systemData\", StructType(Seq(\n",
        "          StructField(\"lastModifiedAt\", StringType),\n",
        "          StructField(\"lastModifiedBy\", StringType),\n",
        "          StructField(\"createdAt\", StringType),\n",
        "          StructField(\"createdBy\", StringType)\n",
        "        )))\n",
        "      )), nullable = true),\n",
        "      StructField(\"after\", StructType(Seq(\n",
        "        StructField(\"id\", StringType),\n",
        "        StructField(\"definition\", StringType),\n",
        "        StructField(\"domain\", StringType),\n",
        "        StructField(\"targetDate\", StringType),\n",
        "        StructField(\"contacts\", StructType(Seq(\n",
        "          StructField(\"owner\", ArrayType(StructType(Seq(\n",
        "            StructField(\"id\", StringType)\n",
        "          )))),\n",
        "        ))),\n",
        "        StructField(\"status\", StringType),\n",
        "        StructField(\"systemData\", StructType(Seq(\n",
        "          StructField(\"lastModifiedAt\", StringType),\n",
        "          StructField(\"lastModifiedBy\", StringType),\n",
        "          StructField(\"createdAt\", StringType),\n",
        "          StructField(\"createdBy\", StringType)\n",
        "        )))\n",
        "      )), nullable = true),\n",
        "      StructField(\"related\", StructType(Seq(\n",
        "      StructField(\"dataAssetId\", ArrayType(StringType))\n",
        "    )), nullable = true)\n",
        "      ))),\n",
        "    StructField(\"eventSource\", StringType),\n",
        "    StructField(\"payloadKind\", StringType),\n",
        "    StructField(\"operationType\", StringType),\n",
        "    StructField(\"preciseTimestamp\", StringType),\n",
        "    StructField(\"tenantId\", StringType),\n",
        "    StructField(\"accountId\", StringType),\n",
        "    StructField(\"changedBy\", StringType),\n",
        "    StructField(\"eventId\", StringType),\n",
        "    StructField(\"correlationId\", StringType),\n",
        "    StructField(\"EventProcessedUtcTime\", StringType),\n",
        "    StructField(\"PartitionId\", IntegerType),\n",
        "    StructField(\"EventEnqueuedUtcTime\", StringType),\n",
        "    StructField(\"id\", StringType),\n",
        "    StructField(\"_rid\", StringType),\n",
        "    StructField(\"_etag\", StringType),\n",
        "    StructField(\"_ts\", LongType)\n",
        "  ))\n",
        "\n",
        "val keyResultContractSchema : StructType = StructType(Seq(\n",
        "    StructField(\"payload\", StructType(Seq(\n",
        "      StructField(\"before\", StructType(Seq(\n",
        "        StructField(\"id\", StringType),\n",
        "        StructField(\"definition\", StringType),\n",
        "        StructField(\"domainId\", StringType),\n",
        "        StructField(\"progress\", IntegerType),\n",
        "        StructField(\"goal\", IntegerType),\n",
        "        StructField(\"max\", IntegerType),\n",
        "        StructField(\"status\", StringType),\n",
        "        StructField(\"systemData\", StructType(Seq(\n",
        "          StructField(\"lastModifiedAt\", StringType),\n",
        "          StructField(\"lastModifiedBy\", StringType),\n",
        "          StructField(\"createdAt\", StringType),\n",
        "          StructField(\"createdBy\", StringType),\n",
        "        )))\n",
        "      )), nullable = true),\n",
        "      StructField(\"after\", StructType(Seq(\n",
        "        StructField(\"id\", StringType),\n",
        "        StructField(\"definition\", StringType),\n",
        "        StructField(\"domainId\", StringType),\n",
        "        StructField(\"progress\", IntegerType),\n",
        "        StructField(\"goal\", IntegerType),\n",
        "        StructField(\"max\", IntegerType),\n",
        "        StructField(\"status\", StringType),\n",
        "        StructField(\"systemData\", StructType(Seq(\n",
        "          StructField(\"lastModifiedAt\", StringType),\n",
        "          StructField(\"lastModifiedBy\", StringType),\n",
        "          StructField(\"createdAt\", StringType),\n",
        "          StructField(\"createdBy\", StringType),\n",
        "        )))\n",
        "      )), nullable = true),\n",
        "      StructField(\"related\", StructType(Seq(\n",
        "        StructField(\"dataAssetId\", ArrayType(StringType))\n",
        "      )), nullable = true)))),\n",
        "    StructField(\"eventSource\", StringType),\n",
        "    StructField(\"payloadKind\", StringType),\n",
        "    StructField(\"operationType\", StringType),\n",
        "    StructField(\"preciseTimestamp\", StringType),\n",
        "    StructField(\"tenantId\", StringType),\n",
        "    StructField(\"accountId\", StringType),\n",
        "    StructField(\"changedBy\", StringType),\n",
        "    StructField(\"eventId\", StringType),\n",
        "    StructField(\"correlationId\", StringType),\n",
        "    StructField(\"EventProcessedUtcTime\", StringType),\n",
        "    StructField(\"PartitionId\", IntegerType),\n",
        "    StructField(\"EventEnqueuedUtcTime\", StringType),\n",
        "    StructField(\"id\", StringType),\n",
        "    StructField(\"_rid\", StringType),\n",
        "    StructField(\"_etag\", StringType),\n",
        "    StructField(\"_ts\", LongType)\n",
        "  ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "testCatalog",
              "statement_id": 7,
              "statement_ids": [
                7
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "7",
              "normalized_state": "finished",
              "queued_time": "2025-03-24T18:08:22.0830533Z",
              "session_start_time": null,
              "execution_start_time": "2025-03-24T18:08:23.9839537Z",
              "execution_finish_time": "2025-03-24T18:09:10.6933287Z",
              "parent_msg_id": "a31ab11c-d2d9-4105-8b1d-02e6c5c18c70"
            },
            "text/plain": "StatementMeta(testCatalog, 7, 7, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting backfill with timestamp comparison for 1 accounts across 3 containers\nProcessing account: 0c89ab95-b9d6-47d8-9101-913ded037ba6, container: cde\nFound 176 records in source for account 0c89ab95-b9d6-47d8-9101-913ded037ba6 in container cde\n[ERROR] [03/24/2025 18:08:46.512] [default-akka.actor.default-dispatcher-21] [akka.actor.ActorSystemImpl(default)] Outgoing request stream error (akka.stream.AbruptTerminationException: Processor actor [Actor[akka://default/system/StreamSupervisor-3/flow-0-0-PoolFlow#1749378738]] terminated abruptly)\nFound 2847 existing payload records (per payload id and max timestamp) in target database for account 0c89ab95-b9d6-47d8-9101-913ded037ba6 in container cde\nAfter filtering, found 0 new or updated records to insert for account 0c89ab95-b9d6-47d8-9101-913ded037ba6 in container cde\nNo new or updated records to insert for account 0c89ab95-b9d6-47d8-9101-913ded037ba6 in container cde\nProcessing account: 0c89ab95-b9d6-47d8-9101-913ded037ba6, container: okr\nFound 214 records in source for account 0c89ab95-b9d6-47d8-9101-913ded037ba6 in container okr\nFound 481 existing payload records (per payload id and max timestamp) in target database for account 0c89ab95-b9d6-47d8-9101-913ded037ba6 in container okr\nAfter filtering, found 0 new or updated records to insert for account 0c89ab95-b9d6-47d8-9101-913ded037ba6 in container okr\nNo new or updated records to insert for account 0c89ab95-b9d6-47d8-9101-913ded037ba6 in container okr\nProcessing account: 0c89ab95-b9d6-47d8-9101-913ded037ba6, container: keyresult\nFound 41 records in source for account 0c89ab95-b9d6-47d8-9101-913ded037ba6 in container keyresult\nFound 155 existing payload records (per payload id and max timestamp) in target database for account 0c89ab95-b9d6-47d8-9101-913ded037ba6 in container keyresult\nAfter filtering, found 25 new or updated records to insert for account 0c89ab95-b9d6-47d8-9101-913ded037ba6 in container keyresult\nSuccessfully wrote 25 records to target database dgh-DataEstateHealth, container keyresult\nBackfill process completed\ngetSchemaForContainer: (containerName: String)org.apache.spark.sql.types.StructType\nloadAndParseJson: (databaseName: String, containerName: String, cosmosEndPoint: String, cosmosKey: String, accountId: String)org.apache.spark.sql.DataFrame\nwriteParsedJsonToDatabaseAndContainer: (parsedDf: org.apache.spark.sql.DataFrame, databaseName: String, containerName: String, cosmosEndPoint: String, cosmosKey: String)Unit\ndefined class PayloadData\nextractPayloadData: (row: org.apache.spark.sql.Row)PayloadData\nextractPayloadId: (row: org.apache.spark.sql.Row)String\ngetExistingPayloadData: (databaseName: String, containerName: String, cosmosEndPoint: String, cosmosKey: String, accountId: String)Map[String,String]\nfilterNewOrUpdatedRecords: (sourceDF: org.apache.spark.sql.DataFrame, existingPayloadData: Map[String,String])org.apache.spark.sql.DataFrame\nperformBackfillWithTimestampComparison: (accountIds: List[String], containers: List[String], sourceDb: String, targetDb: String, cosmosEndPoint: String, cosmosKey: String)Unit\ncosmosEndpoint: String = https://pdgprodcosmosdocdxb.documents.azure.com:443/\ncosmosKey: String = [REDACTED]\naccountIds: List[String] = List(0c89ab95-b9d6-47d8-9101-913ded037ba6)\ncontainers: List[String] = List(cde, okr, keyresult)\nsourceDb: String = dgh-Backfill\ntargetDb: String = dgh-DataEstateHealth\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "// Function to get the appropriate schema for a container\n",
        "def getSchemaForContainer(containerName: String): StructType = {\n",
        "  containerName match {\n",
        "    case \"cde\" => cdeContractSchema\n",
        "    case \"okr\" => objectiveContractSchema\n",
        "    case \"keyresult\" => keyResultContractSchema\n",
        "    case _ => throw new IllegalArgumentException(s\"Unknown container name: $containerName\")\n",
        "  }\n",
        "}\n",
        "\n",
        "// Updated function to load data with account filtering\n",
        "def loadAndParseJson(databaseName: String, containerName: String, cosmosEndPoint: String, cosmosKey: String, accountId: String): DataFrame = {\n",
        "  val schema = getSchemaForContainer(containerName)\n",
        "  \n",
        "  // Load the data from the database\n",
        "  val sourceDF = spark.read.format(\"cosmos.olap\")\n",
        "    .schema(schema)\n",
        "    .option(\"spark.cosmos.accountEndpoint\", cosmosEndPoint)\n",
        "    .option(\"spark.cosmos.database\", databaseName)\n",
        "    .option(\"spark.cosmos.container\", containerName)\n",
        "    .option(\"spark.cosmos.accountKey\", cosmosKey)\n",
        "    .load()\n",
        " \n",
        "  // Filter by account ID\n",
        "  val filteredDF = sourceDF.filter(col(\"accountId\") === accountId)\n",
        "  \n",
        "  // Register the DataFrame as a temporary view\n",
        "  filteredDF.createOrReplaceTempView(s\"source_${containerName}_${accountId.replaceAll(\"-\", \"_\")}\")\n",
        " \n",
        "  // Run a SQL query to select all records\n",
        "  val sqlDF = spark.sql(\n",
        "    s\"\"\"\n",
        "      |SELECT *\n",
        "      |FROM source_${containerName}_${accountId.replaceAll(\"-\", \"_\")}\n",
        "      |ORDER BY id\n",
        "      |\"\"\".stripMargin)\n",
        " \n",
        "  sqlDF\n",
        "}\n",
        "\n",
        "// Updated function to write data to target database\n",
        "def writeParsedJsonToDatabaseAndContainer(parsedDf: DataFrame, databaseName: String, containerName: String, cosmosEndPoint: String, cosmosKey: String): Unit = {\n",
        "    parsedDf.write\n",
        "              .format(\"cosmos.oltp\")\n",
        "              .option(\"spark.cosmos.accountEndpoint\", cosmosEndPoint)\n",
        "              .option(\"spark.cosmos.accountKey\", cosmosKey)\n",
        "              .option(\"spark.cosmos.database\", databaseName)\n",
        "              .option(\"spark.cosmos.container\", containerName)\n",
        "              .option(\"spark.cosmos.write.strategy\", \"ItemOverwrite\")\n",
        "              .mode(\"append\")\n",
        "              .save()\n",
        "}\n",
        "\n",
        "// Enhanced payload data class to hold both ID and lastModifiedAt timestamp\n",
        "case class PayloadData(id: String, lastModifiedAt: String)\n",
        "\n",
        "// Updated helper function to extract both payload ID and lastModifiedAt from a record\n",
        "def extractPayloadData(row: org.apache.spark.sql.Row): PayloadData = {\n",
        "  val payload = row.getStruct(row.fieldIndex(\"payload\"))\n",
        "  \n",
        "  // Try to get data from after, then fall back to before\n",
        "  val afterField = if (payload.isNullAt(payload.fieldIndex(\"after\"))) null else payload.getStruct(payload.fieldIndex(\"after\"))\n",
        "  val beforeField = if (payload.isNullAt(payload.fieldIndex(\"before\"))) null else payload.getStruct(payload.fieldIndex(\"before\"))\n",
        "  \n",
        "  // Extract ID and lastModifiedAt\n",
        "  val id = if (afterField != null && !afterField.isNullAt(afterField.fieldIndex(\"id\"))) {\n",
        "    afterField.getString(afterField.fieldIndex(\"id\"))\n",
        "  } else if (beforeField != null && !beforeField.isNullAt(beforeField.fieldIndex(\"id\"))) {\n",
        "    beforeField.getString(beforeField.fieldIndex(\"id\"))\n",
        "  } else {\n",
        "    null // No ID found in either field\n",
        "  }\n",
        "  \n",
        "  val lastModifiedAt = {\n",
        "    if (afterField != null && !afterField.isNullAt(afterField.fieldIndex(\"systemData\"))) {\n",
        "      val systemData = afterField.getStruct(afterField.fieldIndex(\"systemData\"))\n",
        "      if (!systemData.isNullAt(systemData.fieldIndex(\"lastModifiedAt\"))) {\n",
        "        systemData.getString(systemData.fieldIndex(\"lastModifiedAt\"))\n",
        "      } else null\n",
        "    } else if (beforeField != null && !beforeField.isNullAt(beforeField.fieldIndex(\"systemData\"))) {\n",
        "      val systemData = beforeField.getStruct(beforeField.fieldIndex(\"systemData\"))\n",
        "      if (!systemData.isNullAt(systemData.fieldIndex(\"lastModifiedAt\"))) {\n",
        "        systemData.getString(systemData.fieldIndex(\"lastModifiedAt\"))\n",
        "      } else null\n",
        "    } else {\n",
        "      null\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  PayloadData(id, lastModifiedAt)\n",
        "}\n",
        "\n",
        "// Simplified function to just extract payload ID (for backward compatibility)\n",
        "def extractPayloadId(row: org.apache.spark.sql.Row): String = {\n",
        "  extractPayloadData(row).id\n",
        "}\n",
        "\n",
        "// Function to get existing payload data (ID and lastModifiedAt) from target database\n",
        "def getExistingPayloadData(databaseName: String, containerName: String, cosmosEndPoint: String, cosmosKey: String, accountId: String): Map[String, String] = {\n",
        "  val schema = getSchemaForContainer(containerName)\n",
        "  \n",
        "  try {\n",
        "    // Load the data from the target database\n",
        "    val targetDF = spark.read.format(\"cosmos.olap\")\n",
        "      .schema(schema)\n",
        "      .option(\"spark.cosmos.accountEndpoint\", cosmosEndPoint)\n",
        "      .option(\"spark.cosmos.database\", databaseName)\n",
        "      .option(\"spark.cosmos.container\", containerName)\n",
        "      .option(\"spark.cosmos.accountKey\", cosmosKey)\n",
        "      .load()\n",
        "      .filter(col(\"accountId\") === accountId)\n",
        "    \n",
        "    // Create columns with extracted ID and lastModifiedAt\n",
        "    val idAndTimestampDF = targetDF\n",
        "      .withColumn(\"payload_id\", \n",
        "        coalesce(col(\"payload.after.id\"), col(\"payload.before.id\")))\n",
        "      .withColumn(\"last_modified_at\", \n",
        "        coalesce(col(\"payload.after.systemData.lastModifiedAt\"), \n",
        "                col(\"payload.before.systemData.lastModifiedAt\")))\n",
        "      .filter(col(\"payload_id\").isNotNull && col(\"last_modified_at\").isNotNull)\n",
        "      .select(\"payload_id\", \"last_modified_at\")\n",
        "    \n",
        "    // Group by ID to get the maximum timestamp\n",
        "    val maxTimestampDF = idAndTimestampDF\n",
        "      .groupBy(\"payload_id\")\n",
        "      .agg(max(\"last_modified_at\").as(\"last_modified_at\"))\n",
        "    \n",
        "    // Convert to Map using simple collect approach\n",
        "    val payloadDataMap = maxTimestampDF.collect().map { row =>\n",
        "      (row.getString(0), row.getString(1))\n",
        "    }.toMap\n",
        "    \n",
        "    println(s\"Found ${payloadDataMap.size} existing payload records (per payload id and max timestamp) in target database for account $accountId in container $containerName\")\n",
        "    payloadDataMap\n",
        "  } catch {\n",
        "    case e: Exception => \n",
        "      println(s\"Error fetching existing payload data from target: ${e.getMessage}\")\n",
        "      e.printStackTrace()\n",
        "      Map.empty[String, String]\n",
        "  }\n",
        "}\n",
        "\n",
        "// Updated function to filter source records and set operationType to \"Update\" for existing records\n",
        "def filterNewOrUpdatedRecords(sourceDF: DataFrame, existingPayloadData: Map[String, String]): DataFrame = {\n",
        "  // Create a UDF to extract both payload ID and lastModifiedAt\n",
        "  val extractPayloadDataUdf = udf((payload: org.apache.spark.sql.Row) => {\n",
        "    val afterField = if (payload.isNullAt(payload.fieldIndex(\"after\"))) null else payload.getStruct(payload.fieldIndex(\"after\"))\n",
        "    val beforeField = if (payload.isNullAt(payload.fieldIndex(\"before\"))) null else payload.getStruct(payload.fieldIndex(\"before\"))\n",
        "    \n",
        "    // Extract ID\n",
        "    val id = if (afterField != null && !afterField.isNullAt(afterField.fieldIndex(\"id\"))) {\n",
        "      afterField.getString(afterField.fieldIndex(\"id\"))\n",
        "    } else if (beforeField != null && !beforeField.isNullAt(beforeField.fieldIndex(\"id\"))) {\n",
        "      beforeField.getString(beforeField.fieldIndex(\"id\"))\n",
        "    } else {\n",
        "      null\n",
        "    }\n",
        "    \n",
        "    // Extract lastModifiedAt\n",
        "    val lastModifiedAt = {\n",
        "      if (afterField != null && !afterField.isNullAt(afterField.fieldIndex(\"systemData\"))) {\n",
        "        val systemData = afterField.getStruct(afterField.fieldIndex(\"systemData\"))\n",
        "        if (!systemData.isNullAt(systemData.fieldIndex(\"lastModifiedAt\"))) {\n",
        "          systemData.getString(systemData.fieldIndex(\"lastModifiedAt\"))\n",
        "        } else null\n",
        "      } else if (beforeField != null && !beforeField.isNullAt(beforeField.fieldIndex(\"systemData\"))) {\n",
        "        val systemData = beforeField.getStruct(beforeField.fieldIndex(\"systemData\"))\n",
        "        if (!systemData.isNullAt(systemData.fieldIndex(\"lastModifiedAt\"))) {\n",
        "          systemData.getString(systemData.fieldIndex(\"lastModifiedAt\"))\n",
        "        } else null\n",
        "      } else {\n",
        "        null\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    (id, lastModifiedAt)\n",
        "  })\n",
        "  \n",
        "  // Create a UDF to determine if a record should be included (either new or more recent)\n",
        "  // Also return if it's an update (exists but newer) or create (new)\n",
        "  val processRecordUdf = udf((idAndTimestamp: org.apache.spark.sql.Row) => {\n",
        "    if (idAndTimestamp == null) {\n",
        "      (false, null) // Skip record, not an update\n",
        "    } else {\n",
        "      val id = idAndTimestamp.getString(0)\n",
        "      val lastModifiedAt = idAndTimestamp.getString(1)\n",
        "      \n",
        "      if (id == null || lastModifiedAt == null) {\n",
        "        (false, null) // Skip record, not an update\n",
        "      } else if (!existingPayloadData.contains(id)) {\n",
        "        // If ID doesn't exist in target, include it as a Create\n",
        "        (true, \"Create\")\n",
        "      } else {\n",
        "        // If ID exists, compare lastModifiedAt timestamps\n",
        "        val existingLastModifiedAt = existingPayloadData(id)\n",
        "        \n",
        "        // Include if source lastModifiedAt is more recent than target\n",
        "        if (lastModifiedAt > existingLastModifiedAt) {\n",
        "          (true, \"Update\") // Include and mark as Update\n",
        "        } else {\n",
        "          (false, null) // Skip record, not newer\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  })\n",
        "  \n",
        "  // Add columns with the extracted data and process record decision\n",
        "  val processedDF = sourceDF\n",
        "    .withColumn(\"extractedPayloadData\", extractPayloadDataUdf(col(\"payload\")))\n",
        "    .withColumn(\"processRecord\", processRecordUdf(col(\"extractedPayloadData\")))\n",
        "    \n",
        "  // Filter records to include and update operationType if it's an existing record\n",
        "  val filteredDF = processedDF\n",
        "    .filter(col(\"processRecord._1\") === true) // Only include records that passed the filter\n",
        "    .withColumn(\"operationType\", \n",
        "      when(col(\"processRecord._2\") === \"Update\", lit(\"Update\"))\n",
        "      .otherwise(col(\"operationType\")) // Keep original value for new records\n",
        "    )\n",
        "    .drop(\"extractedPayloadData\", \"processRecord\")\n",
        "  \n",
        "  filteredDF\n",
        "}\n",
        "\n",
        "// Updated backfill function to use the enhanced filtering logic\n",
        "def performBackfillWithTimestampComparison(accountIds: List[String], containers: List[String], sourceDb: String, targetDb: String, cosmosEndPoint: String, cosmosKey: String): Unit = {\n",
        "  println(s\"Starting backfill with timestamp comparison for ${accountIds.size} accounts across ${containers.size} containers\")\n",
        "  \n",
        "  for {\n",
        "    accountId <- accountIds\n",
        "    container <- containers\n",
        "  } {\n",
        "    println(s\"Processing account: $accountId, container: $container\")\n",
        "    try {\n",
        "      // Load data from source\n",
        "      val sourceData = loadAndParseJson(sourceDb, container, cosmosEndPoint, cosmosKey, accountId)\n",
        "      val sourceCount = sourceData.count()\n",
        "      println(s\"Found $sourceCount records in source for account $accountId in container $container\")\n",
        "      \n",
        "      if (sourceCount > 0) {\n",
        "        // Get existing payload data from target\n",
        "        val existingPayloadData = getExistingPayloadData(targetDb, container, cosmosEndPoint, cosmosKey, accountId)\n",
        "        \n",
        "        // Filter records that are either new or have a more recent lastModifiedAt\n",
        "        val recordsToInsert = filterNewOrUpdatedRecords(sourceData, existingPayloadData)\n",
        "        val recordsToInsertCount = recordsToInsert.count()\n",
        "        \n",
        "        println(s\"After filtering, found $recordsToInsertCount new or updated records to insert for account $accountId in container $container\")\n",
        "        \n",
        "        if (recordsToInsertCount > 0) {\n",
        "          // Write new/updated records to target\n",
        "          writeParsedJsonToDatabaseAndContainer(recordsToInsert, targetDb, container, cosmosEndPoint, cosmosKey)\n",
        "          println(s\"Successfully wrote $recordsToInsertCount records to target database $targetDb, container $container\")\n",
        "        } else {\n",
        "          println(s\"No new or updated records to insert for account $accountId in container $container\")\n",
        "        }\n",
        "      } else {\n",
        "        println(s\"No records found in source for account $accountId in container $container - skipping\")\n",
        "      }\n",
        "    } catch {\n",
        "      case e: Exception => \n",
        "        println(s\"Error processing account $accountId, container $container: ${e.getMessage}\")\n",
        "        e.printStackTrace()\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  println(\"Backfill process completed\")\n",
        "}\n",
        "\n",
        "// Example usage with the enhanced timestamp comparison logic:\n",
        "val cosmosEndpoint = \"https://pdgprodcosmosdocdxb.documents.azure.com:443/\"\n",
        "val cosmosKey = mssparkutils.credentials.getSecret(\"dgh-prod-dxb-kv\", \"cosmosDBWritekey\")\n",
        "\n",
        "// List of account IDs to process\n",
        "val accountIds = List(\n",
        "  \"0c89ab95-b9d6-47d8-9101-913ded037ba6\",\n",
        "\"83b87287-22e3-4d36-990a-865e123ece55\"\n",
        ")\n",
        "\n",
        "// List of containers to process\n",
        "val containers = List(\"cde\", \"okr\", \"keyresult\")\n",
        "\n",
        "// Source and target databases\n",
        "val sourceDb = \"dgh-Backfill\"\n",
        "val targetDb = \"dgh-DataEstateHealth\"\n",
        "\n",
        "// Execute the backfill with timestamp comparison\n",
        "performBackfillWithTimestampComparison(accountIds, containers, sourceDb, targetDb, cosmosEndpoint, cosmosKey)"
      ]
    }
  ]
}