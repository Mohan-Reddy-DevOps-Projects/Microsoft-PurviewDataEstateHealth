{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.types._\n",
        "\n",
        "// Keep all the existing schema definitions and supporting functions from the original code\n",
        "// (cdeContractSchema, objectiveContractSchema, keyResultContractSchema, getSchemaForContainer, etc.)\n",
        "\n",
        "val cdeContractSchema: StructType = StructType(Seq(\n",
        "  StructField(\"payload\", StructType(Seq(\n",
        "    StructField(\"before\", StructType(Seq(\n",
        "      StructField(\"name\", StringType),\n",
        "      StructField(\"dataType\", StringType),\n",
        "      StructField(\"status\", StringType),\n",
        "      StructField(\"contacts\", StructType(Seq(\n",
        "        StructField(\"owner\", ArrayType(StructType(Seq(\n",
        "          StructField(\"id\", StringType)\n",
        "        ))))\n",
        "      ))),\n",
        "      StructField(\"description\", StringType),\n",
        "      StructField(\"domain\", StringType),\n",
        "      StructField(\"id\", StringType),\n",
        "      StructField(\"systemData\", StructType(Seq(\n",
        "        StructField(\"lastModifiedAt\", StringType),\n",
        "        StructField(\"lastModifiedBy\", StringType),\n",
        "        StructField(\"createdAt\", StringType),\n",
        "        StructField(\"createdBy\", StringType)\n",
        "      )))\n",
        "    )), nullable = true),  // Set before field as nullable\n",
        "    StructField(\"after\", StructType(Seq(\n",
        "      StructField(\"name\", StringType),\n",
        "      StructField(\"dataType\", StringType),\n",
        "      StructField(\"status\", StringType),\n",
        "      StructField(\"contacts\", StructType(Seq(\n",
        "        StructField(\"owner\", ArrayType(StructType(Seq(\n",
        "          StructField(\"id\", StringType)\n",
        "        ))))\n",
        "      ))),\n",
        "      StructField(\"description\", StringType),\n",
        "      StructField(\"domain\", StringType),\n",
        "      StructField(\"id\", StringType),\n",
        "      StructField(\"systemData\", StructType(Seq(\n",
        "        StructField(\"lastModifiedAt\", StringType),\n",
        "        StructField(\"lastModifiedBy\", StringType),\n",
        "        StructField(\"createdAt\", StringType),\n",
        "        StructField(\"createdBy\", StringType)\n",
        "      )))\n",
        "    )), nullable = true),  // Set after field as nullable\n",
        "    StructField(\"related\", StructType(Seq(\n",
        "      StructField(\"dataAssetId\", ArrayType(StringType))\n",
        "    )), nullable = true)  // Also making related nullable for consistency\n",
        "  ))),\n",
        "  StructField(\"eventSource\", StringType),\n",
        "  StructField(\"payloadKind\", StringType),\n",
        "  StructField(\"operationType\", StringType),\n",
        "  StructField(\"preciseTimestamp\", StringType),\n",
        "  StructField(\"tenantId\", StringType),\n",
        "  StructField(\"accountId\", StringType),\n",
        "  StructField(\"changedBy\", StringType),\n",
        "  StructField(\"eventId\", StringType),\n",
        "  StructField(\"correlationId\", StringType),\n",
        "  StructField(\"EventProcessedUtcTime\", StringType),\n",
        "  StructField(\"PartitionId\", IntegerType),\n",
        "  StructField(\"EventEnqueuedUtcTime\", StringType),\n",
        "  StructField(\"id\", StringType),\n",
        "  StructField(\"_rid\", StringType),\n",
        "  StructField(\"_etag\", StringType),\n",
        "  StructField(\"_ts\", LongType)\n",
        "))\n",
        "\n",
        "\n",
        "val objectiveContractSchema : StructType = StructType(Seq(\n",
        "    StructField(\"payload\", StructType(Seq(\n",
        "      StructField(\"before\", StructType(Seq(\n",
        "        StructField(\"id\", StringType),\n",
        "        StructField(\"definition\", StringType),\n",
        "        StructField(\"domain\", StringType),\n",
        "        StructField(\"targetDate\", StringType),\n",
        "        StructField(\"contacts\", StructType(Seq(\n",
        "          StructField(\"owner\", ArrayType(StructType(Seq(\n",
        "            StructField(\"id\", StringType)\n",
        "          )))),\n",
        "        ))),\n",
        "        StructField(\"status\", StringType),\n",
        "        StructField(\"systemData\", StructType(Seq(\n",
        "          StructField(\"lastModifiedAt\", StringType),\n",
        "          StructField(\"lastModifiedBy\", StringType),\n",
        "          StructField(\"createdAt\", StringType),\n",
        "          StructField(\"createdBy\", StringType)\n",
        "        )))\n",
        "      )), nullable = true),\n",
        "      StructField(\"after\", StructType(Seq(\n",
        "        StructField(\"id\", StringType),\n",
        "        StructField(\"definition\", StringType),\n",
        "        StructField(\"domain\", StringType),\n",
        "        StructField(\"targetDate\", StringType),\n",
        "        StructField(\"contacts\", StructType(Seq(\n",
        "          StructField(\"owner\", ArrayType(StructType(Seq(\n",
        "            StructField(\"id\", StringType)\n",
        "          )))),\n",
        "        ))),\n",
        "        StructField(\"status\", StringType),\n",
        "        StructField(\"systemData\", StructType(Seq(\n",
        "          StructField(\"lastModifiedAt\", StringType),\n",
        "          StructField(\"lastModifiedBy\", StringType),\n",
        "          StructField(\"createdAt\", StringType),\n",
        "          StructField(\"createdBy\", StringType)\n",
        "        )))\n",
        "      )), nullable = true),\n",
        "      StructField(\"related\", StructType(Seq(\n",
        "      StructField(\"dataAssetId\", ArrayType(StringType))\n",
        "    )), nullable = true)\n",
        "      ))),\n",
        "    StructField(\"eventSource\", StringType),\n",
        "    StructField(\"payloadKind\", StringType),\n",
        "    StructField(\"operationType\", StringType),\n",
        "    StructField(\"preciseTimestamp\", StringType),\n",
        "    StructField(\"tenantId\", StringType),\n",
        "    StructField(\"accountId\", StringType),\n",
        "    StructField(\"changedBy\", StringType),\n",
        "    StructField(\"eventId\", StringType),\n",
        "    StructField(\"correlationId\", StringType),\n",
        "    StructField(\"EventProcessedUtcTime\", StringType),\n",
        "    StructField(\"PartitionId\", IntegerType),\n",
        "    StructField(\"EventEnqueuedUtcTime\", StringType),\n",
        "    StructField(\"id\", StringType),\n",
        "    StructField(\"_rid\", StringType),\n",
        "    StructField(\"_etag\", StringType),\n",
        "    StructField(\"_ts\", LongType)\n",
        "  ))\n",
        "\n",
        "val keyResultContractSchema : StructType = StructType(Seq(\n",
        "    StructField(\"payload\", StructType(Seq(\n",
        "      StructField(\"before\", StructType(Seq(\n",
        "        StructField(\"id\", StringType),\n",
        "        StructField(\"definition\", StringType),\n",
        "        StructField(\"domainId\", StringType),\n",
        "        StructField(\"progress\", IntegerType),\n",
        "        StructField(\"goal\", IntegerType),\n",
        "        StructField(\"max\", IntegerType),\n",
        "        StructField(\"status\", StringType),\n",
        "        StructField(\"systemData\", StructType(Seq(\n",
        "          StructField(\"lastModifiedAt\", StringType),\n",
        "          StructField(\"lastModifiedBy\", StringType),\n",
        "          StructField(\"createdAt\", StringType),\n",
        "          StructField(\"createdBy\", StringType),\n",
        "        )))\n",
        "      )), nullable = true),\n",
        "      StructField(\"after\", StructType(Seq(\n",
        "        StructField(\"id\", StringType),\n",
        "        StructField(\"definition\", StringType),\n",
        "        StructField(\"domainId\", StringType),\n",
        "        StructField(\"progress\", IntegerType),\n",
        "        StructField(\"goal\", IntegerType),\n",
        "        StructField(\"max\", IntegerType),\n",
        "        StructField(\"status\", StringType),\n",
        "        StructField(\"systemData\", StructType(Seq(\n",
        "          StructField(\"lastModifiedAt\", StringType),\n",
        "          StructField(\"lastModifiedBy\", StringType),\n",
        "          StructField(\"createdAt\", StringType),\n",
        "          StructField(\"createdBy\", StringType),\n",
        "        )))\n",
        "      )), nullable = true),\n",
        "      StructField(\"related\", StructType(Seq(\n",
        "        StructField(\"dataAssetId\", ArrayType(StringType))\n",
        "      )), nullable = true)))),\n",
        "    StructField(\"eventSource\", StringType),\n",
        "    StructField(\"payloadKind\", StringType),\n",
        "    StructField(\"operationType\", StringType),\n",
        "    StructField(\"preciseTimestamp\", StringType),\n",
        "    StructField(\"tenantId\", StringType),\n",
        "    StructField(\"accountId\", StringType),\n",
        "    StructField(\"changedBy\", StringType),\n",
        "    StructField(\"eventId\", StringType),\n",
        "    StructField(\"correlationId\", StringType),\n",
        "    StructField(\"EventProcessedUtcTime\", StringType),\n",
        "    StructField(\"PartitionId\", IntegerType),\n",
        "    StructField(\"EventEnqueuedUtcTime\", StringType),\n",
        "    StructField(\"id\", StringType),\n",
        "    StructField(\"_rid\", StringType),\n",
        "    StructField(\"_etag\", StringType),\n",
        "    StructField(\"_ts\", LongType)\n",
        "  ))\n",
        "\n",
        "def getSchemaForContainer(containerName: String): StructType = {\n",
        "  containerName match {\n",
        "    case \"cde\" => cdeContractSchema\n",
        "    case \"okr\" => objectiveContractSchema\n",
        "    case \"keyresult\" => keyResultContractSchema\n",
        "    case _ => throw new IllegalArgumentException(s\"Unknown container name: $containerName\")\n",
        "  }\n",
        "}\n",
        "\n",
        "def loadAndParseJson(databaseName: String, containerName: String, cosmosEndPoint: String, cosmosKey: String, accountId: String): DataFrame = {\n",
        "  val schema = getSchemaForContainer(containerName)\n",
        "  \n",
        "  // Load the data from the database\n",
        "  val sourceDF = spark.read.format(\"cosmos.olap\")\n",
        "    .schema(schema)\n",
        "    .option(\"spark.cosmos.accountEndpoint\", cosmosEndPoint)\n",
        "    .option(\"spark.cosmos.database\", databaseName)\n",
        "    .option(\"spark.cosmos.container\", containerName)\n",
        "    .option(\"spark.cosmos.accountKey\", cosmosKey)\n",
        "    .load()\n",
        " \n",
        "  // Filter by account ID\n",
        "  val filteredDF = sourceDF.filter(col(\"accountId\") === accountId)\n",
        "  \n",
        "  filteredDF\n",
        "}\n",
        "// Create a case class to store validation results\n",
        "case class ValidationResult(\n",
        "  accountId: String,\n",
        "  containerName: String,\n",
        "  sourceCount: Long,\n",
        "  targetCount: Long,\n",
        "  inSourceNotInTarget: Long,\n",
        "  inTargetNotInSource: Long,\n",
        "  deletedInTarget: Long,\n",
        "  distinctInTargetNotInSourceNotDeleted: Long\n",
        ")\n",
        "\n",
        "// Function to run validation for a single accountId and container\n",
        "def validateSingleAccountContainer(\n",
        "  accountId: String,\n",
        "  containerName: String,\n",
        "  sourceDatabaseName: String,\n",
        "  targetDatabaseName: String,\n",
        "  cosmosEndPoint: String,\n",
        "  cosmosKey: String\n",
        "): ValidationResult = {\n",
        "  \n",
        "  println(s\"Running validation for accountId: $accountId, container: $containerName\")\n",
        "  \n",
        "  try {\n",
        "    // Load source and target data\n",
        "    val sourceViewName = s\"source_${containerName}_${accountId.replaceAll(\"-\", \"_\")}\"\n",
        "    val targetViewName = s\"target_${containerName}_${accountId.replaceAll(\"-\", \"_\")}\"\n",
        "    \n",
        "    // Load the data frames\n",
        "    val sourceDf = loadAndParseJson(sourceDatabaseName, containerName, cosmosEndPoint, cosmosKey, accountId)\n",
        "    val targetDf = loadAndParseJson(targetDatabaseName, containerName, cosmosEndPoint, cosmosKey, accountId)\n",
        "    \n",
        "    // Create temp views\n",
        "    sourceDf.createOrReplaceTempView(sourceViewName)\n",
        "    targetDf.createOrReplaceTempView(targetViewName)\n",
        "    \n",
        "    // Get counts\n",
        "    val sourceCount = sourceDf.count()\n",
        "    val targetCount = targetDf.count()\n",
        "    \n",
        "    // Find records in source but not in target\n",
        "    val allInSourceNotInTarget = spark.sql(s\"\"\"\n",
        "      SELECT s.* \n",
        "      FROM ${sourceViewName} s\n",
        "      WHERE NOT EXISTS (\n",
        "        SELECT 1 \n",
        "        FROM ${targetViewName} t\n",
        "        WHERE s.payload.after.id = t.payload.after.id\n",
        "      )\n",
        "    \"\"\")\n",
        "    val inSourceNotInTargetCount = allInSourceNotInTarget.count()\n",
        "    \n",
        "    // Find records in target but not in source\n",
        "    val allInTargetNotInSource = spark.sql(s\"\"\"\n",
        "      SELECT s.* \n",
        "      FROM ${targetViewName} s\n",
        "      WHERE NOT EXISTS (\n",
        "        SELECT 1 \n",
        "        FROM ${sourceViewName} t\n",
        "        WHERE s.payload.after.id = t.payload.after.id\n",
        "      )\n",
        "    \"\"\")\n",
        "    val inTargetNotInSourceCount = allInTargetNotInSource.count()\n",
        "    \n",
        "    // Find deleted records in target\n",
        "    val allInTargetThatAreDelete = spark.sql(s\"\"\"\n",
        "      SELECT s.* \n",
        "      FROM ${targetViewName} s\n",
        "      WHERE s.operationType = \"Delete\"\n",
        "    \"\"\")\n",
        "    val deletedInTargetCount = allInTargetThatAreDelete.count()\n",
        "    \n",
        "    // Find records in target not in source and not deleted\n",
        "    allInTargetNotInSource.createOrReplaceTempView(\"AllInTargetNotInSource\")\n",
        "    allInTargetThatAreDelete.createOrReplaceTempView(\"AllInTargetThatAreDelete\")\n",
        "    \n",
        "    val allInTargetNotInSourceButNotDeleted = spark.sql(s\"\"\"\n",
        "      SELECT s.payload.after.id \n",
        "      FROM AllInTargetNotInSource s\n",
        "      LEFT JOIN AllInTargetThatAreDelete t\n",
        "      ON s.payload.before.id = t.payload.before.id\n",
        "      OR s.payload.after.id = t.payload.before.id\n",
        "      WHERE t.id is NULL\n",
        "    \"\"\")\n",
        "    val distinctInTargetNotInSourceNotDeletedCount = allInTargetNotInSourceButNotDeleted.distinct().count()\n",
        "    \n",
        "    // Clean up temporary views to avoid conflicts in subsequent runs\n",
        "    spark.catalog.dropTempView(sourceViewName)\n",
        "    spark.catalog.dropTempView(targetViewName)\n",
        "    spark.catalog.dropTempView(\"AllInTargetNotInSource\")\n",
        "    spark.catalog.dropTempView(\"AllInTargetThatAreDelete\")\n",
        "    \n",
        "    // Return validation result\n",
        "    ValidationResult(\n",
        "      accountId,\n",
        "      containerName,\n",
        "      sourceCount,\n",
        "      targetCount,\n",
        "      inSourceNotInTargetCount,\n",
        "      inTargetNotInSourceCount,\n",
        "      deletedInTargetCount,\n",
        "      distinctInTargetNotInSourceNotDeletedCount\n",
        "    )\n",
        "  } catch {\n",
        "    case e: Exception => \n",
        "      println(s\"Error validating account $accountId, container $containerName: ${e.getMessage}\")\n",
        "      e.printStackTrace()\n",
        "      ValidationResult(\n",
        "        accountId,\n",
        "        containerName,\n",
        "        -1, // Use -1 to indicate error in counts\n",
        "        -1,\n",
        "        -1,\n",
        "        -1,\n",
        "        -1,\n",
        "        -1\n",
        "      )\n",
        "  }\n",
        "}\n",
        "\n",
        "// Main function to run validation across multiple accounts and containers\n",
        "def runBatchValidation(\n",
        "  accountIds: List[String],\n",
        "  containers: List[String],\n",
        "  sourceDatabaseName: String,\n",
        "  targetDatabaseName: String,\n",
        "  cosmosEndPoint: String,\n",
        "  cosmosKey: String\n",
        "): DataFrame = {\n",
        "  \n",
        "  import spark.implicits._\n",
        "  \n",
        "  println(s\"Starting batch validation for ${accountIds.size} accounts across ${containers.size} containers\")\n",
        "  println(s\"Source DB: $sourceDatabaseName, Target DB: $targetDatabaseName\")\n",
        "  \n",
        "  // Create a list to store all validation results\n",
        "  var allResults = List[ValidationResult]()\n",
        "  \n",
        "  // Process each account and container\n",
        "  for {\n",
        "    accountId <- accountIds\n",
        "    container <- containers\n",
        "  } {\n",
        "    val result = validateSingleAccountContainer(\n",
        "      accountId,\n",
        "      container, \n",
        "      sourceDatabaseName,\n",
        "      targetDatabaseName,\n",
        "      cosmosEndPoint,\n",
        "      cosmosKey\n",
        "    )\n",
        "    \n",
        "    allResults = allResults :+ result\n",
        "  }\n",
        "  \n",
        "  // Convert results to DataFrame\n",
        "  val resultsDF = allResults.toDF()\n",
        "  \n",
        "  // Display results\n",
        "  println(\"Validation complete. Results summary:\")\n",
        "  resultsDF.show(false)\n",
        "  \n",
        "  // Return the DataFrame for further processing if needed\n",
        "  resultsDF\n",
        "}\n",
        "\n",
        "// Example usage - replace with your actual values\n",
        "val sourceDatabaseName = \"dgh-Backfill\"\n",
        "val targetDatabaseName = \"dgh-DataEstateHealth\"\n",
        "val cosmosEndPoint = \"https://pdgprodcosmosdocdxb.documents.azure.com:443/\"\n",
        "val cosmosKey = mssparkutils.credentials.getSecret(\"dgh-prod-dxb-kv\", \"cosmosDBWritekey\")\n",
        "\n",
        "// Define list of accountIds and containers to validate\n",
        "val accountIds = List(\n",
        "  \"0c89ab95-b9d6-47d8-9101-913ded037ba6\",\n",
        "\"83b87287-22e3-4d36-990a-865e123ece55\"\n",
        ")\n",
        "\n",
        "val containers = List(\n",
        "  \"cde\",\n",
        "  \"okr\",\n",
        "  \"keyresult\"\n",
        ")\n",
        "\n",
        "// Run the validation\n",
        "val validationResults = runBatchValidation(\n",
        "  accountIds,\n",
        "  containers,\n",
        "  sourceDatabaseName,\n",
        "  targetDatabaseName,\n",
        "  cosmosEndPoint,\n",
        "  cosmosKey\n",
        ")\n",
        "\n",
        "display(validationResults)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "testCatalog",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "14",
              "normalized_state": "finished",
              "queued_time": "2025-03-26T03:35:25.631197Z",
              "session_start_time": null,
              "execution_start_time": "2025-03-26T03:35:25.6334283Z",
              "execution_finish_time": "2025-03-26T03:35:52.5921477Z",
              "parent_msg_id": "928f5efb-5edb-445b-a09b-5b8640590370"
            },
            "text/plain": "StatementMeta(testCatalog, 14, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting batch validation for 2 accounts across 3 containers\nSource DB: dgh-Backfill, Target DB: dgh-DataEstateHealth\nRunning validation for accountId: 0c89ab95-b9d6-47d8-9101-913ded037ba6, container: cde\nRunning validation for accountId: 0c89ab95-b9d6-47d8-9101-913ded037ba6, container: okr\nRunning validation for accountId: 0c89ab95-b9d6-47d8-9101-913ded037ba6, container: keyresult\nRunning validation for accountId: 83b87287-22e3-4d36-990a-865e123ece55, container: cde\nRunning validation for accountId: 83b87287-22e3-4d36-990a-865e123ece55, container: okr\nRunning validation for accountId: 83b87287-22e3-4d36-990a-865e123ece55, container: keyresult\n[ERROR] [03/26/2025 03:35:48.845] [default-akka.actor.default-dispatcher-7] [akka.actor.ActorSystemImpl(default)] Outgoing request stream error (akka.stream.AbruptTerminationException: Processor actor [Actor[akka://default/system/StreamSupervisor-93/flow-0-0-PoolFlow#-885755594]] terminated abruptly)\n[ERROR] [03/26/2025 03:35:48.847] [default-akka.actor.default-dispatcher-6] [akka.actor.ActorSystemImpl(default)] Outgoing request stream error (akka.stream.AbruptTerminationException: Processor actor [Actor[akka://default/system/StreamSupervisor-93/flow-3-0-PoolFlow#-1815019970]] terminated abruptly)\nValidation complete. Results summary:\n+------------------------------------+-------------+-----------+-----------+-------------------+-------------------+---------------+-------------------------------------+\n|accountId                           |containerName|sourceCount|targetCount|inSourceNotInTarget|inTargetNotInSource|deletedInTarget|distinctInTargetNotInSourceNotDeleted|\n+------------------------------------+-------------+-----------+-----------+-------------------+-------------------+---------------+-------------------------------------+\n|0c89ab95-b9d6-47d8-9101-913ded037ba6|cde          |175        |8396       |0                  |7978               |2667           |19                                   |\n|0c89ab95-b9d6-47d8-9101-913ded037ba6|okr          |213        |1155       |0                  |900                |281            |277                                  |\n|0c89ab95-b9d6-47d8-9101-913ded037ba6|keyresult    |40         |344        |0                  |299                |84             |144                                  |\n|83b87287-22e3-4d36-990a-865e123ece55|cde          |93         |5706       |0                  |5576               |1811           |9                                    |\n|83b87287-22e3-4d36-990a-865e123ece55|okr          |65         |1058       |0                  |993                |285            |285                                  |\n|83b87287-22e3-4d36-990a-865e123ece55|keyresult    |7          |260        |0                  |253                |59             |105                                  |\n+------------------------------------+-------------+-----------+-----------+-------------------+-------------------+---------------+-------------------------------------+\n\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "88fd0902-9a30-4f5e-ab56-9a2528526eb8",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 88fd0902-9a30-4f5e-ab56-9a2528526eb8)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nimport org.apache.spark.sql.{SparkSession, DataFrame}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\ncdeContractSchema: org.apache.spark.sql.types.StructType = StructType(StructField(payload,StructType(StructField(before,StructType(StructField(name,StringType,true),StructField(dataType,StringType,true),StructField(status,StringType,true),StructField(contacts,StructType(StructField(owner,ArrayType(StructType(StructField(id,StringType,true)),true),true)),true),StructField(description,StringType,true),StructField(domain,StringType,true),StructField(id,StringType,true),StructField(systemData,StructType(StructField(lastModifiedAt,StringType,true),StructField(lastModifiedBy,StringType,true),StructField(createdAt,StringType,true),StructField(createdBy,StringType,true)),true)),true),StructField(after,StructType(StructField(name,StringType,true),StructField(dataType,StringType,true),StructField...\nobjectiveContractSchema: org.apache.spark.sql.types.StructType = StructType(StructField(payload,StructType(StructField(before,StructType(StructField(id,StringType,true),StructField(definition,StringType,true),StructField(domain,StringType,true),StructField(targetDate,StringType,true),StructField(contacts,StructType(StructField(owner,ArrayType(StructType(StructField(id,StringType,true)),true),true)),true),StructField(status,StringType,true),StructField(systemData,StructType(StructField(lastModifiedAt,StringType,true),StructField(lastModifiedBy,StringType,true),StructField(createdAt,StringType,true),StructField(createdBy,StringType,true)),true)),true),StructField(after,StructType(StructField(id,StringType,true),StructField(definition,StringType,true),StructField(domain,StringType,true),St...\nkeyResultContractSchema: org.apache.spark.sql.types.StructType = StructType(StructField(payload,StructType(StructField(before,StructType(StructField(id,StringType,true),StructField(definition,StringType,true),StructField(domainId,StringType,true),StructField(progress,IntegerType,true),StructField(goal,IntegerType,true),StructField(max,IntegerType,true),StructField(status,StringType,true),StructField(systemData,StructType(StructField(lastModifiedAt,StringType,true),StructField(lastModifiedBy,StringType,true),StructField(createdAt,StringType,true),StructField(createdBy,StringType,true)),true)),true),StructField(after,StructType(StructField(id,StringType,true),StructField(definition,StringType,true),StructField(domainId,StringType,true),StructField(progress,IntegerType,true),StructField(go...\ngetSchemaForContainer: (containerName: String)org.apache.spark.sql.types.StructType\nloadAndParseJson: (databaseName: String, containerName: String, cosmosEndPoint: String, cosmosKey: String, accountId: String)org.apache.spark.sql.DataFrame\ndefined class ValidationResult\nvalidateSingleAccountContainer: (accountId: String, containerName: String, sourceDatabaseName: String, targetDatabaseName: String, cosmosEndPoint: String, cosmosKey: String)ValidationResult\nrunBatchValidation: (accountIds: List[String], containers: List[String], sourceDatabaseName: String, targetDatabaseName: String, cosmosEndPoint: String, cosmosKey: String)org.apache.spark.sql.DataFrame\nsourceDatabaseName: String = dgh-Backfill\ntargetDatabaseName: String = dgh-DataEstateHealth\ncosmosEndPoint: String = https://pdgprodcosmosdocdxb.documents.azure.com:443/\ncosmosKey: String = [REDACTED]\naccountIds: List[String] = List(0c89ab95-b9d6-47d8-9101-913ded037ba6, 83b87287-22e3-4d36-990a-865e123ece55)\ncontainers: List[String] = List(cde, okr, keyresult)\nvalidationResults: org.apache.spark.sql.DataFrame = [accountId: string, containerName: string ... 6 more fields]\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "scala"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {
        "88fd0902-9a30-4f5e-ab56-9a2528526eb8": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "0c89ab95-b9d6-47d8-9101-913ded037ba6",
                  "1": "cde",
                  "2": "175",
                  "3": "8396",
                  "4": "0",
                  "5": "7978",
                  "6": "2667",
                  "7": "19"
                },
                {
                  "0": "0c89ab95-b9d6-47d8-9101-913ded037ba6",
                  "1": "okr",
                  "2": "213",
                  "3": "1155",
                  "4": "0",
                  "5": "900",
                  "6": "281",
                  "7": "277"
                },
                {
                  "0": "0c89ab95-b9d6-47d8-9101-913ded037ba6",
                  "1": "keyresult",
                  "2": "40",
                  "3": "344",
                  "4": "0",
                  "5": "299",
                  "6": "84",
                  "7": "144"
                },
                {
                  "0": "83b87287-22e3-4d36-990a-865e123ece55",
                  "1": "cde",
                  "2": "93",
                  "3": "5706",
                  "4": "0",
                  "5": "5576",
                  "6": "1811",
                  "7": "9"
                },
                {
                  "0": "83b87287-22e3-4d36-990a-865e123ece55",
                  "1": "okr",
                  "2": "65",
                  "3": "1058",
                  "4": "0",
                  "5": "993",
                  "6": "285",
                  "7": "285"
                },
                {
                  "0": "83b87287-22e3-4d36-990a-865e123ece55",
                  "1": "keyresult",
                  "2": "7",
                  "3": "260",
                  "4": "0",
                  "5": "253",
                  "6": "59",
                  "7": "105"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "accountId",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "containerName",
                  "type": "string"
                },
                {
                  "key": "2",
                  "name": "sourceCount",
                  "type": "bigint"
                },
                {
                  "key": "3",
                  "name": "targetCount",
                  "type": "bigint"
                },
                {
                  "key": "4",
                  "name": "inSourceNotInTarget",
                  "type": "bigint"
                },
                {
                  "key": "5",
                  "name": "inTargetNotInSource",
                  "type": "bigint"
                },
                {
                  "key": "6",
                  "name": "deletedInTarget",
                  "type": "bigint"
                },
                {
                  "key": "7",
                  "name": "distinctInTargetNotInSourceNotDeleted",
                  "type": "bigint"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "2"
                ],
                "isStacked": false
              }
            }
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}